{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark exploration\n",
    "\n",
    "This notebook contains exploratatory queries into data for the following tests runs run between 2020-08-24 and 2020-08-25.\n",
    "\n",
    "```bash\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=32 N_ROWS=2500 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=32 N_ROWS=5000 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=32 N_ROWS=7500 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=32 N_ROWS=10000 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=64 N_ROWS=2500 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=64 N_ROWS=5000 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=64 N_ROWS=7500 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=64 N_ROWS=10000 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=128 N_ROWS=2500 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=128 N_ROWS=5000 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=128 N_ROWS=7500 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "BUCKET=gs://prio-processor-benchmark  N_DATA=128 N_ROWS=10000 MACHINE_TYPE=n1-standard-16 NUM_WORKERS=0 ./scripts/test-cli-integration-dataproc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mtest-128-10000-n1-standard-16-0-20200824003830\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-128-2500-n1-standard-16-0-20200824004122\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-128-5000-n1-standard-16-0-20200824004027\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-128-7500-n1-standard-16-0-20200824003959\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-32-10000-n1-standard-16-0-20200824000750396484\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-32-2500-n1-standard-16-0-20200823234848898312\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-32-5000-n1-standard-16-0-20200824000733009240\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-32-7500-n1-standard-16-0-20200824000741901558\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-512-10000-n1-standard-16-0-20200825231625\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-512-100000-n1-standard-4-0-20200825223401\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-512-1000000-n1-standard-4-0-20200825222931\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-512-1000000-n1-standard-4-0-20200825223019\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-64-10000-n1-standard-16-0-20200824001934367765\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-64-2500-n1-standard-16-0-20200824001916185249\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-64-5000-n1-standard-16-0-20200824001922568161\u001b[m\u001b[m\r\n",
      "\u001b[34mtest-64-7500-n1-standard-16-0-20200824001928540460\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts from a single trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/test-128-10000-n1-standard-16-0-20200824003830\u001b[00m\r\n",
      "├── \u001b[01;34mclient\u001b[00m\r\n",
      "│   ├── _SUCCESS\r\n",
      "│   ├── \u001b[01;34mserver_id=a\u001b[00m\r\n",
      "│   │   └── part-00000-faab898b-0423-4e16-813b-560ac71d754d.c000.json\r\n",
      "│   └── \u001b[01;34mserver_id=b\u001b[00m\r\n",
      "│       └── part-00000-faab898b-0423-4e16-813b-560ac71d754d.c000.json\r\n",
      "├── config.json\r\n",
      "├── \u001b[01;34mlogs\u001b[00m\r\n",
      "│   ├── bucket-listing.txt\r\n",
      "│   ├── dataproc-clusters-describe.json\r\n",
      "│   ├── dataproc-jobs-list.json\r\n",
      "│   ├── \u001b[01;34mspark-job-history\u001b[00m\r\n",
      "│   │   ├── application_1598254988496_0002\r\n",
      "│   │   ├── application_1598254988496_0003\r\n",
      "│   │   ├── application_1598254988496_0004\r\n",
      "│   │   ├── application_1598254988496_0005\r\n",
      "│   │   ├── application_1598254988496_0006\r\n",
      "│   │   ├── application_1598254988496_0007\r\n",
      "│   │   ├── application_1598254988496_0008\r\n",
      "│   │   ├── application_1598254988496_0009\r\n",
      "│   │   └── application_1598254988496_0010\r\n",
      "│   └── \u001b[01;34myarn-logs\u001b[00m\r\n",
      "│       └── \u001b[01;34mroot\u001b[00m\r\n",
      "│           └── \u001b[01;34mlogs-tfile\u001b[00m\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0002\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0003\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0004\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0005\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0006\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0007\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0008\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               ├── \u001b[01;34mapplication_1598254988496_0009\u001b[00m\r\n",
      "│               │   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "│               └── \u001b[01;34mapplication_1598254988496_0010\u001b[00m\r\n",
      "│                   └── test-128-10000-n1-standard-16-0-20200824003830-m.c.amiyaguchi-dev.internal_8026\r\n",
      "├── \u001b[01;34mserver_a\u001b[00m\r\n",
      "│   └── \u001b[01;34mprocessed\u001b[00m\r\n",
      "│       ├── _SUCCESS\r\n",
      "│       └── part-00000-59df14d1-6b8d-4a64-bee5-6c750866aed7-c000.json\r\n",
      "├── server_a_keys.json\r\n",
      "├── \u001b[01;34mserver_b\u001b[00m\r\n",
      "│   └── \u001b[01;34mprocessed\u001b[00m\r\n",
      "│       ├── _SUCCESS\r\n",
      "│       └── part-00000-464d4322-471b-49c8-bff6-caa2f5e3524a-c000.json\r\n",
      "├── server_b_keys.json\r\n",
      "└── shared_seed.json\r\n",
      "\r\n",
      "21 directories, 32 files\r\n"
     ]
    }
   ],
   "source": [
    "! tree ../data/test-128-10000-n1-standard-16-0-20200824003830"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+------+-----------+-----+--------------------+\n",
      "|batch_id|  machine_type|n_data|n_rows|num_workers|scale|          cluster_id|\n",
      "+--------+--------------+------+------+-----------+-----+--------------------+\n",
      "|    test| n1-standard-4|   512| 10000|          0|  100|test-512-1000000-...|\n",
      "|    test| n1-standard-4|   512| 10000|          0|  100|test-512-1000000-...|\n",
      "|    test|n1-standard-16|   128| 10000|          0|    1|test-128-10000-n1...|\n",
      "|    test| n1-standard-4|   512| 10000|          0|   10|test-512-100000-n...|\n",
      "|    test|n1-standard-16|   512| 10000|          0|    1|test-512-10000-n1...|\n",
      "|    test|n1-standard-16|   128|  5000|          0|    1|test-128-5000-n1-...|\n",
      "|    test|n1-standard-16|   128|  2500|          0|    1|test-128-2500-n1-...|\n",
      "|    test|n1-standard-16|    32| 10000|          0|    1|test-32-10000-n1-...|\n",
      "|    test|n1-standard-16|   128|  7500|          0|    1|test-128-7500-n1-...|\n",
      "|    test|n1-standard-16|    64| 10000|          0|    1|test-64-10000-n1-...|\n",
      "|    test|n1-standard-16|    64|  7500|          0|    1|test-64-7500-n1-s...|\n",
      "|    test|n1-standard-16|    64|  5000|          0|    1|test-64-5000-n1-s...|\n",
      "|    test|n1-standard-16|    32|  7500|          0|    1|test-32-7500-n1-s...|\n",
      "|    test|n1-standard-16|    32|  2500|          0|    1|test-32-2500-n1-s...|\n",
      "|    test|n1-standard-16|    32|  5000|          0|    1|test-32-5000-n1-s...|\n",
      "|    test|n1-standard-16|    64|  2500|          0|    1|test-64-2500-n1-s...|\n",
      "+--------+--------------+------+------+-----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(\"string\")\n",
    "def parse_config_path(filename):\n",
    "    parts = filename.split(\"/\")\n",
    "    return parts[-2]\n",
    "\n",
    "config = (\n",
    "    spark.read.json(\"../data/*/config.json\", multiLine=True)\n",
    "    .withColumn(\"cluster_id\", parse_config_path(F.input_file_name()))\n",
    ")\n",
    "config.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataproc job listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- done: boolean (nullable = true)\n",
      " |-- driverControlFilesUri: string (nullable = true)\n",
      " |-- driverOutputResourceUri: string (nullable = true)\n",
      " |-- jobUuid: string (nullable = true)\n",
      " |-- placement: struct (nullable = true)\n",
      " |    |-- clusterName: string (nullable = true)\n",
      " |    |-- clusterUuid: string (nullable = true)\n",
      " |-- pysparkJob: struct (nullable = true)\n",
      " |    |-- args: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- mainPythonFileUri: string (nullable = true)\n",
      " |    |-- pythonFileUris: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- reference: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- projectId: string (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- stateStartTime: timestamp (nullable = true)\n",
      " |-- statusHistory: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- details: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- stateStartTime: timestamp (nullable = true)\n",
      " |-- submittedBy: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- yarnApplications: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- progress: double (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- trackingUrl: string (nullable = true)\n",
      "\n",
      "-RECORD 0---------------------------------------------------------------------------------------------------\n",
      " done                    | true                                                                             \n",
      " driverControlFilesUri   | gs://dataproc-8ef2a382-154c-4479-8c6f-790740e63509-us-west1/google-cloud-data... \n",
      " driverOutputResourceUri | gs://dataproc-8ef2a382-154c-4479-8c6f-790740e63509-us-west1/google-cloud-data... \n",
      " jobUuid                 | 25149c66-523c-37e1-b7c0-bb59eaf0fb7f                                             \n",
      " placement               | [test-32-10000-n1-standard-16-0-20200824000750396484, 4326264f-a2ec-4d83-95ee... \n",
      " pysparkJob              | [[publish, --n-data, 32, --batch-id, test, --server-id, B, --private-key-hex,... \n",
      " reference               | [c65af90f25ea498aa9c18af40273a562, amiyaguchi-dev]                               \n",
      " status                  | [DONE, 2020-08-24 00:17:43.686]                                                  \n",
      " statusHistory           | [[, PENDING, 2020-08-24 00:17:09.083], [, SETUP_DONE, 2020-08-24 00:17:09.117... \n",
      " submittedBy             | amiyaguchi@mozilla.com                                                           \n",
      " type                    | pyspark                                                                          \n",
      " yarnApplications        | [[processor.py, 1.0, FINISHED, test-32-10000-n1-standard-16-0-202008240007503... \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------\n",
      " done                    | true                                                                             \n",
      " driverControlFilesUri   | gs://dataproc-8ef2a382-154c-4479-8c6f-790740e63509-us-west1/google-cloud-data... \n",
      " driverOutputResourceUri | gs://dataproc-8ef2a382-154c-4479-8c6f-790740e63509-us-west1/google-cloud-data... \n",
      " jobUuid                 | 17945512-7dd7-394f-8a50-9709d4ab3441                                             \n",
      " placement               | [test-32-10000-n1-standard-16-0-20200824000750396484, 4326264f-a2ec-4d83-95ee... \n",
      " pysparkJob              | [[publish, --n-data, 32, --batch-id, test, --server-id, A, --private-key-hex,... \n",
      " reference               | [61b825d05d7f462f978b477112252cf2, amiyaguchi-dev]                               \n",
      " status                  | [DONE, 2020-08-24 00:17:03.64]                                                   \n",
      " statusHistory           | [[, PENDING, 2020-08-24 00:16:30.428], [, SETUP_DONE, 2020-08-24 00:16:30.464... \n",
      " submittedBy             | amiyaguchi@mozilla.com                                                           \n",
      " type                    | pyspark                                                                          \n",
      " yarnApplications        | [[processor.py, 1.0, FINISHED, test-32-10000-n1-standard-16-0-202008240007503... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_list = spark.read.json(\"../data/*/*/dataproc-jobs-list.json\", multiLine=True)\n",
    "job_list.printSchema()\n",
    "job_list.show(vertical=True, n=2, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                            data|\n",
      "+--------------------------------------------------------------------------------+\n",
      "|         0  2020-08-24T07:44:49Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|       237  2020-08-24T07:38:36Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|      1784  2020-08-24T07:38:36Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|     91112  2020-08-24T07:38:37Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|        65  2020-08-24T07:38:37Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|      1383  2020-08-24T07:38:37Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:44:49Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:45:20Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:45:17Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|  63922860  2020-08-24T07:45:18Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:45:17Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|   2669968  2020-08-24T07:45:18Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:45:54Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:45:54Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:54:55Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|      2095  2020-08-24T07:54:55Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:48:25Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|    910000  2020-08-24T07:48:24Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|         0  2020-08-24T07:51:34Z  gs://prio-processor-benchmark/data/working/...|\n",
      "|    750000  2020-08-24T07:51:34Z  gs://prio-processor-benchmark/data/working/...|\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket_listing = spark.read.csv(\"../data/*/*/bucket-listing.txt\", sep=\"\\t\", schema=\"data string\")\n",
    "bucket_listing.show(truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Spark Job History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- App ID: string (nullable = true)\n",
      " |-- App Name: string (nullable = true)\n",
      " |-- Block Manager ID: struct (nullable = true)\n",
      " |    |-- Executor ID: string (nullable = true)\n",
      " |    |-- Host: string (nullable = true)\n",
      " |    |-- Port: long (nullable = true)\n",
      " |-- Completion Time: long (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Executor ID: string (nullable = true)\n",
      " |-- Executor Info: struct (nullable = true)\n",
      " |    |-- Attributes: struct (nullable = true)\n",
      " |    |    |-- CLUSTER_ID: string (nullable = true)\n",
      " |    |    |-- CONTAINER_ID: string (nullable = true)\n",
      " |    |    |-- HTTP_SCHEME: string (nullable = true)\n",
      " |    |    |-- LOG_FILES: string (nullable = true)\n",
      " |    |    |-- NM_HOST: string (nullable = true)\n",
      " |    |    |-- NM_HTTP_ADDRESS: string (nullable = true)\n",
      " |    |    |-- NM_HTTP_PORT: string (nullable = true)\n",
      " |    |    |-- NM_PORT: string (nullable = true)\n",
      " |    |    |-- USER: string (nullable = true)\n",
      " |    |-- Host: string (nullable = true)\n",
      " |    |-- Log Urls: struct (nullable = true)\n",
      " |    |    |-- stderr: string (nullable = true)\n",
      " |    |    |-- stdout: string (nullable = true)\n",
      " |    |-- Total Cores: long (nullable = true)\n",
      " |-- JVM Information: struct (nullable = true)\n",
      " |    |-- Java Home: string (nullable = true)\n",
      " |    |-- Java Version: string (nullable = true)\n",
      " |    |-- Scala Version: string (nullable = true)\n",
      " |-- Job ID: long (nullable = true)\n",
      " |-- Job Result: struct (nullable = true)\n",
      " |    |-- Result: string (nullable = true)\n",
      " |-- Maximum Memory: long (nullable = true)\n",
      " |-- Maximum Offheap Memory: long (nullable = true)\n",
      " |-- Maximum Onheap Memory: long (nullable = true)\n",
      " |-- Removed Reason: string (nullable = true)\n",
      " |-- Spark Version: string (nullable = true)\n",
      " |-- Stage Attempt ID: long (nullable = true)\n",
      " |-- Stage ID: long (nullable = true)\n",
      " |-- Stage IDs: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Stage Info: struct (nullable = true)\n",
      " |    |-- Accumulables: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Count Failed Values: boolean (nullable = true)\n",
      " |    |    |    |-- ID: long (nullable = true)\n",
      " |    |    |    |-- Internal: boolean (nullable = true)\n",
      " |    |    |    |-- Metadata: string (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- Value: string (nullable = true)\n",
      " |    |-- Completion Time: long (nullable = true)\n",
      " |    |-- Details: string (nullable = true)\n",
      " |    |-- Failure Reason: string (nullable = true)\n",
      " |    |-- Number of Tasks: long (nullable = true)\n",
      " |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- RDD Info: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Barrier: boolean (nullable = true)\n",
      " |    |    |    |-- Callsite: string (nullable = true)\n",
      " |    |    |    |-- Disk Size: long (nullable = true)\n",
      " |    |    |    |-- Memory Size: long (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- Number of Cached Partitions: long (nullable = true)\n",
      " |    |    |    |-- Number of Partitions: long (nullable = true)\n",
      " |    |    |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- RDD ID: long (nullable = true)\n",
      " |    |    |    |-- Scope: string (nullable = true)\n",
      " |    |    |    |-- Storage Level: struct (nullable = true)\n",
      " |    |    |    |    |-- Deserialized: boolean (nullable = true)\n",
      " |    |    |    |    |-- Replication: long (nullable = true)\n",
      " |    |    |    |    |-- Use Disk: boolean (nullable = true)\n",
      " |    |    |    |    |-- Use Memory: boolean (nullable = true)\n",
      " |    |-- Stage Attempt ID: long (nullable = true)\n",
      " |    |-- Stage ID: long (nullable = true)\n",
      " |    |-- Stage Name: string (nullable = true)\n",
      " |    |-- Submission Time: long (nullable = true)\n",
      " |-- Stage Infos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Accumulables: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- Details: string (nullable = true)\n",
      " |    |    |-- Number of Tasks: long (nullable = true)\n",
      " |    |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- RDD Info: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- Barrier: boolean (nullable = true)\n",
      " |    |    |    |    |-- Callsite: string (nullable = true)\n",
      " |    |    |    |    |-- Disk Size: long (nullable = true)\n",
      " |    |    |    |    |-- Memory Size: long (nullable = true)\n",
      " |    |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |    |-- Number of Cached Partitions: long (nullable = true)\n",
      " |    |    |    |    |-- Number of Partitions: long (nullable = true)\n",
      " |    |    |    |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- RDD ID: long (nullable = true)\n",
      " |    |    |    |    |-- Scope: string (nullable = true)\n",
      " |    |    |    |    |-- Storage Level: struct (nullable = true)\n",
      " |    |    |    |    |    |-- Deserialized: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- Replication: long (nullable = true)\n",
      " |    |    |    |    |    |-- Use Disk: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- Use Memory: boolean (nullable = true)\n",
      " |    |    |-- Stage Attempt ID: long (nullable = true)\n",
      " |    |    |-- Stage ID: long (nullable = true)\n",
      " |    |    |-- Stage Name: string (nullable = true)\n",
      " |-- Submission Time: long (nullable = true)\n",
      " |-- Task End Reason: struct (nullable = true)\n",
      " |    |-- Block Manager Address: struct (nullable = true)\n",
      " |    |    |-- Executor ID: string (nullable = true)\n",
      " |    |    |-- Host: string (nullable = true)\n",
      " |    |    |-- Port: long (nullable = true)\n",
      " |    |-- Map ID: long (nullable = true)\n",
      " |    |-- Map Index: long (nullable = true)\n",
      " |    |-- Message: string (nullable = true)\n",
      " |    |-- Reason: string (nullable = true)\n",
      " |    |-- Reduce ID: long (nullable = true)\n",
      " |    |-- Shuffle ID: long (nullable = true)\n",
      " |-- Task Executor Metrics: struct (nullable = true)\n",
      " |    |-- DirectPoolMemory: long (nullable = true)\n",
      " |    |-- JVMHeapMemory: long (nullable = true)\n",
      " |    |-- JVMOffHeapMemory: long (nullable = true)\n",
      " |    |-- MajorGCCount: long (nullable = true)\n",
      " |    |-- MajorGCTime: long (nullable = true)\n",
      " |    |-- MappedPoolMemory: long (nullable = true)\n",
      " |    |-- MinorGCCount: long (nullable = true)\n",
      " |    |-- MinorGCTime: long (nullable = true)\n",
      " |    |-- OffHeapExecutionMemory: long (nullable = true)\n",
      " |    |-- OffHeapStorageMemory: long (nullable = true)\n",
      " |    |-- OffHeapUnifiedMemory: long (nullable = true)\n",
      " |    |-- OnHeapExecutionMemory: long (nullable = true)\n",
      " |    |-- OnHeapStorageMemory: long (nullable = true)\n",
      " |    |-- OnHeapUnifiedMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeJVMRSSMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeJVMVMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeOtherRSSMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeOtherVMemory: long (nullable = true)\n",
      " |    |-- ProcessTreePythonRSSMemory: long (nullable = true)\n",
      " |    |-- ProcessTreePythonVMemory: long (nullable = true)\n",
      " |-- Task Info: struct (nullable = true)\n",
      " |    |-- Accumulables: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Count Failed Values: boolean (nullable = true)\n",
      " |    |    |    |-- ID: long (nullable = true)\n",
      " |    |    |    |-- Internal: boolean (nullable = true)\n",
      " |    |    |    |-- Metadata: string (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- Update: string (nullable = true)\n",
      " |    |    |    |-- Value: string (nullable = true)\n",
      " |    |-- Attempt: long (nullable = true)\n",
      " |    |-- Executor ID: string (nullable = true)\n",
      " |    |-- Failed: boolean (nullable = true)\n",
      " |    |-- Finish Time: long (nullable = true)\n",
      " |    |-- Getting Result Time: long (nullable = true)\n",
      " |    |-- Host: string (nullable = true)\n",
      " |    |-- Index: long (nullable = true)\n",
      " |    |-- Killed: boolean (nullable = true)\n",
      " |    |-- Launch Time: long (nullable = true)\n",
      " |    |-- Locality: string (nullable = true)\n",
      " |    |-- Speculative: boolean (nullable = true)\n",
      " |    |-- Task ID: long (nullable = true)\n",
      " |-- Task Metrics: struct (nullable = true)\n",
      " |    |-- Disk Bytes Spilled: long (nullable = true)\n",
      " |    |-- Executor CPU Time: long (nullable = true)\n",
      " |    |-- Executor Deserialize CPU Time: long (nullable = true)\n",
      " |    |-- Executor Deserialize Time: long (nullable = true)\n",
      " |    |-- Executor Run Time: long (nullable = true)\n",
      " |    |-- Input Metrics: struct (nullable = true)\n",
      " |    |    |-- Bytes Read: long (nullable = true)\n",
      " |    |    |-- Records Read: long (nullable = true)\n",
      " |    |-- JVM GC Time: long (nullable = true)\n",
      " |    |-- Memory Bytes Spilled: long (nullable = true)\n",
      " |    |-- Output Metrics: struct (nullable = true)\n",
      " |    |    |-- Bytes Written: long (nullable = true)\n",
      " |    |    |-- Records Written: long (nullable = true)\n",
      " |    |-- Peak Execution Memory: long (nullable = true)\n",
      " |    |-- Result Serialization Time: long (nullable = true)\n",
      " |    |-- Result Size: long (nullable = true)\n",
      " |    |-- Shuffle Read Metrics: struct (nullable = true)\n",
      " |    |    |-- Fetch Wait Time: long (nullable = true)\n",
      " |    |    |-- Local Blocks Fetched: long (nullable = true)\n",
      " |    |    |-- Local Bytes Read: long (nullable = true)\n",
      " |    |    |-- Remote Blocks Fetched: long (nullable = true)\n",
      " |    |    |-- Remote Bytes Read: long (nullable = true)\n",
      " |    |    |-- Remote Bytes Read To Disk: long (nullable = true)\n",
      " |    |    |-- Total Records Read: long (nullable = true)\n",
      " |    |-- Shuffle Write Metrics: struct (nullable = true)\n",
      " |    |    |-- Shuffle Bytes Written: long (nullable = true)\n",
      " |    |    |-- Shuffle Records Written: long (nullable = true)\n",
      " |    |    |-- Shuffle Write Time: long (nullable = true)\n",
      " |    |-- Updated Blocks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Task Type: string (nullable = true)\n",
      " |-- Timestamp: long (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- accumUpdates: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- details: string (nullable = true)\n",
      " |-- executionId: long (nullable = true)\n",
      " |-- physicalPlanDescription: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- filename: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_history = (\n",
    "    spark.read.json(\"../data/*/logs/spark-job-history/*\")\n",
    "    .drop(\"sparkPlanInfo\")\n",
    "    .drop(\"Hadoop Properties\")\n",
    "    .drop(\"Spark Properties\")\n",
    "    .drop(\"Properties\")\n",
    "    .drop(\"Classpath Entries\")\n",
    "    .drop(\"System Properties\")\n",
    "    .withColumn(\"filename\", F.input_file_name())\n",
    ")\n",
    "job_history.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+\n",
      "|parse_benchmark_path(filename)                                                      |\n",
      "+------------------------------------------------------------------------------------+\n",
      "|[test-32-7500-n1-standard-16-0-20200824000741901558, application_1598252956150_0007]|\n",
      "|[test-32-7500-n1-standard-16-0-20200824000741901558, application_1598252956150_0008]|\n",
      "|[test-128-10000-n1-standard-16-0-20200824003830, application_1598254988496_0003]    |\n",
      "|[test-64-2500-n1-standard-16-0-20200824001916185249, application_1598253651488_0005]|\n",
      "|[test-32-7500-n1-standard-16-0-20200824000741901558, application_1598252956150_0002]|\n",
      "+------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by file convention, we can extract both the cluster-id and application name for grouping\n",
    "\n",
    "@F.udf(\"struct<cluster_id: string, application_name: string>\")\n",
    "def parse_benchmark_path(filename):\n",
    "    \"\"\"Parse the spark job history filename for relevant grouping information.\n",
    "    file:/Users/amiyaguchi/Work/prio-processor/data/test-32-2500-n1-standard-16-0-20200823234848898312/logs/spark-job-history/application_1598251821633_0002\n",
    "    \"\"\"\n",
    "    parts = filename.split(\"/\")\n",
    "    return dict(cluster_id=parts[-4], application_name=parts[-1])\n",
    "\n",
    "job_history.select(parse_benchmark_path(\"filename\")).distinct().show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------+----------+-------+-----------+-----------+-----------+-----------------+-----------------+-----------+\n",
      "| reason|stage_id|stage_attempt_id| task_type|task_id|executor_id|run_seconds|cpu_seconds|deser_run_seconds|deser_cpu_seconds|result_size|\n",
      "+-------+--------+----------------+----------+-------+-----------+-----------+-----------+-----------------+-----------------+-----------+\n",
      "|Success|       0|               0|ResultTask|      0|          2|      2.204|1.072734103|            0.843|      0.340503745|       1966|\n",
      "|Success|       0|               0|ResultTask|      1|          2|      2.284|1.065204953|            0.842|      0.561023979|       1966|\n",
      "|Success|       0|               0|ResultTask|      2|          2|      2.283|0.825324974|            0.843|      0.416646733|       1966|\n",
      "|Success|       0|               0|ResultTask|      3|          2|      2.003| 0.57048707|            0.842|      0.356277139|       1966|\n",
      "|Success|       1|               0|ResultTask|      4|          1|      9.275|1.229807144|            0.802|      0.412887988|       3292|\n",
      "|Success|       1|               0|ResultTask|      5|          2|      8.365|0.198724914|            0.148|      0.101162726|       3249|\n",
      "|Success|       1|               0|ResultTask|      6|          1|      9.951|  0.6884893|            0.802|      0.622080002|       3249|\n",
      "|Success|       1|               0|ResultTask|      7|          2|      5.164|0.494408354|            0.149|      0.062994096|       3292|\n",
      "+-------+--------+----------------+----------+-------+-----------+-----------+-----------+-----------------+-----------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics\n",
    "task_metrics = (\n",
    "    job_history\n",
    "    .where(F.col(\"Event\") == \"SparkListenerTaskEnd\")\n",
    "    .withColumn(\"job_info\", parse_benchmark_path(\"filename\"))\n",
    "    .withColumn(\"task_info\", F.col(\"Task Info\"))\n",
    "    .withColumn(\"task_metrics\", F.col(\"Task Metrics\"))\n",
    "    .select(\"*\", \"task_metrics.*\")\n",
    "    .select(\n",
    "        \"job_info.*\",\n",
    "        F.col(\"Task End Reason\").getField(\"Reason\").alias(\"reason\"),\n",
    "        F.col(\"Stage ID\").alias(\"stage_id\"),\n",
    "        F.col(\"Stage Attempt ID\").alias(\"stage_attempt_id\"),\n",
    "        F.col(\"Task Type\").alias(\"task_type\"), \n",
    "        F.col(\"task_info\").getField(\"Task ID\").alias(\"task_id\"),\n",
    "        F.col(\"task_info\").getField(\"Executor ID\").alias(\"executor_id\"),\n",
    "        (F.col(\"Executor Run Time\")/1000).alias(\"run_seconds\"),\n",
    "        (F.col(\"Executor CPU Time\")/10**9).alias(\"cpu_seconds\"), \n",
    "        (F.col(\"Executor Deserialize Time\")/1000).alias(\"deser_run_seconds\"),\n",
    "        (F.col(\"Executor Deserialize CPU Time\")/10**9).alias(\"deser_cpu_seconds\"),\n",
    "        F.col(\"Result Size\").alias(\"result_size\"),\n",
    "    )\n",
    "    .orderBy(\"cluster_id\", \"application_name\", \"task_id\", \"executor_id\")\n",
    ")\n",
    "\n",
    "single_perf = task_metrics.where(\n",
    "    \"cluster_id='test-32-7500-n1-standard-16-0-20200824000741901558'\"\n",
    "    \"AND application_name='application_1598252956150_0003'\"\n",
    ").drop(\"cluster_id\", \"application_name\")\n",
    "\n",
    "single_perf.show()\n",
    "single_perf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|     reason|count|\n",
      "+-----------+-----+\n",
      "|FetchFailed|  131|\n",
      "|    Success| 8890|\n",
      "+-----------+-----+\n",
      "\n",
      "+----------------+-----+\n",
      "|stage_attempt_id|count|\n",
      "+----------------+-----+\n",
      "|               0| 5689|\n",
      "|               1| 2301|\n",
      "|               3|    5|\n",
      "|               2| 1026|\n",
      "+----------------+-----+\n",
      "\n",
      "+----------------+-----------+-----+\n",
      "|stage_attempt_id|     reason|count|\n",
      "+----------------+-----------+-----+\n",
      "|               0|FetchFailed|  108|\n",
      "|               0|    Success| 5581|\n",
      "|               1|FetchFailed|   23|\n",
      "|               1|    Success| 2278|\n",
      "|               2|    Success| 1026|\n",
      "|               3|    Success|    5|\n",
      "+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_metrics.groupBy(\"reason\").count().show()\n",
    "task_metrics.groupBy(\"stage_attempt_id\").count().show()\n",
    "task_metrics.groupBy(\"stage_attempt_id\", \"reason\").count().orderBy(\"stage_attempt_id\", \"reason\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+\n",
      "|sequence_id|stage_id|       run_seconds|\n",
      "+-----------+--------+------------------+\n",
      "|       0002|       3|            1898.0|\n",
      "|       0003|       1|             767.6|\n",
      "|       0004|       1|402.09999999999997|\n",
      "|       0005|       5|             765.4|\n",
      "|       0006|       5|             398.0|\n",
      "|       0007|       6| 985.8000000000001|\n",
      "|       0008|       6|             501.0|\n",
      "|       0009|       3|              43.6|\n",
      "|       0010|       3| 42.89999999999999|\n",
      "+-----------+--------+------------------+\n",
      "\n",
      "+-----------+--------+--------------------+-----------+-----------+-----------+----------+------+------+\n",
      "|sequence_id|stage_id|          cluster_id|run_seconds|cpu_seconds|result_size|task_count|n_data|n_rows|\n",
      "+-----------+--------+--------------------+-----------+-----------+-----------+----------+------+------+\n",
      "|       0002|       3|test-32-2500-n1-s...|       87.0|        4.6|     870853|       219|    32|  2500|\n",
      "|       0002|       3|test-64-2500-n1-s...|      111.3|        4.3|     859112|       216|    64|  2500|\n",
      "|       0002|       3|test-128-2500-n1-...|      138.4|        4.5|     845446|       213|   128|  2500|\n",
      "|       0002|       3|test-32-5000-n1-s...|      110.5|        4.2|     854926|       215|    32|  5000|\n",
      "|       0002|       3|test-64-5000-n1-s...|      125.2|        4.2|     846984|       213|    64|  5000|\n",
      "|       0002|       3|test-128-5000-n1-...|      201.3|        4.3|     858505|       216|   128|  5000|\n",
      "|       0002|       3|test-32-7500-n1-s...|      109.4|        4.3|     863040|       217|    32|  7500|\n",
      "|       0002|       3|test-64-7500-n1-s...|      152.9|        4.0|     835286|       210|    64|  7500|\n",
      "|       0002|       3|test-128-7500-n1-...|      248.7|        4.4|     846263|       213|   128|  7500|\n",
      "|       0002|       3|test-32-10000-n1-...|      114.5|        4.0|     823193|       207|    32| 10000|\n",
      "|       0002|       3|test-64-10000-n1-...|      191.8|        4.6|     847240|       213|    64| 10000|\n",
      "|       0002|       3|test-128-10000-n1...|      307.0|        4.9|     838671|       211|   128| 10000|\n",
      "|       0003|       1|test-32-2500-n1-s...|       12.5|        1.1|       6584|         2|    32|  2500|\n",
      "|       0003|       1|test-64-2500-n1-s...|       17.0|        1.2|       6584|         2|    64|  2500|\n",
      "|       0003|       1|test-128-2500-n1-...|       37.5|        1.4|      13082|         4|   128|  2500|\n",
      "|       0003|       1|test-32-5000-n1-s...|       20.7|        1.2|       9833|         3|    32|  5000|\n",
      "|       0003|       1|test-64-5000-n1-s...|       38.0|        2.5|      13082|         4|    64|  5000|\n",
      "|       0003|       1|test-128-5000-n1-...|       74.7|        1.7|      26078|         8|   128|  5000|\n",
      "|       0003|       1|test-32-7500-n1-s...|       32.8|        2.6|      13082|         4|    32|  7500|\n",
      "|       0003|       1|test-64-7500-n1-s...|       54.7|        1.7|      19580|         6|    64|  7500|\n",
      "+-----------+--------+--------------------+-----------+-----------+-----------+----------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(\"string\")\n",
    "def sequence_id(application_name):\n",
    "    return application_name.split(\"_\")[-1]\n",
    "\n",
    "# deser_run_seconds, deser_cpu_seconds\n",
    "metrics = [\"run_seconds\", \"cpu_seconds\", \"result_size\"]\n",
    "task_aggregation = (\n",
    "    task_metrics\n",
    "    .withColumn(\"sequence_id\", sequence_id(\"application_name\"))\n",
    "    .where(\"reason = 'Success'\")\n",
    "    .groupBy(\"sequence_id\", \"stage_id\", \"cluster_id\")\n",
    "    .agg(*[F.round(F.sum(metric), 1).alias(metric) for metric in metrics], \n",
    "         F.countDistinct(\"task_id\", \"executor_id\").alias(\"task_count\"))\n",
    "    .orderBy(\"sequence_id\", \"stage_id\", \"cluster_id\")\n",
    ")\n",
    "task_aggregation.cache()\n",
    "\n",
    "# now we only keep the stage for each sequence with the highest cumulative value\n",
    "# since this is the stage of interest for scaling\n",
    "largest_stage = (\n",
    "    task_aggregation\n",
    "    .groupBy(\"sequence_id\", \"stage_id\")\n",
    "    .agg(F.sum(\"run_seconds\").alias(\"run_seconds\"))\n",
    "    .withColumn(\n",
    "        \"_rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"sequence_id\")\n",
    "            .orderBy(F.desc(\"run_seconds\"))\n",
    "        )\n",
    "    )\n",
    "    .where(\"_rank=1\")\n",
    "    .drop(\"_rank\")\n",
    "    .orderBy(\"sequence_id\")\n",
    ")\n",
    "largest_stage.show()\n",
    "\n",
    "task_aggregation_results = (\n",
    "    task_aggregation\n",
    "    .join(config.select(\"cluster_id\", \"n_data\", \"n_rows\"), on=\"cluster_id\")\n",
    "    .join(largest_stage.drop(\"run_seconds\"), on=[\"sequence_id\", \"stage_id\"], how=\"right\")\n",
    "    .orderBy(\"sequence_id\", \"stage_id\", \"n_rows\", \"n_data\")\n",
    ")\n",
    "\n",
    "task_aggregation_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+------+-----------+\n",
      "|server_id|sequence_id|n_data|n_rows|run_seconds|\n",
      "+---------+-----------+------+------+-----------+\n",
      "|        a|          0|    32|  2500|       12.5|\n",
      "|        a|          0|    64|  2500|       17.0|\n",
      "|        a|          0|   128|  2500|       37.5|\n",
      "|        a|          0|    32|  5000|       20.7|\n",
      "|        a|          0|    64|  5000|       38.0|\n",
      "+---------+-----------+------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_server_id = (\n",
    "    task_aggregation_results\n",
    "    .withColumn(\"sequence_id\", F.col(\"sequence_id\").cast(\"int\"))\n",
    "    .where(\"sequence_id > 2\")\n",
    "    .withColumn(\"server_id\", F.when(F.col(\"sequence_id\")%2==1, F.lit(\"a\")).otherwise(F.lit(\"b\")))\n",
    "    .withColumn(\"sequence_id\", ((F.col(\"sequence_id\")-3)/2).cast(\"int\"))\n",
    "    .select(\"server_id\", \"sequence_id\", \"n_data\", \"n_rows\", \"run_seconds\")\n",
    ")\n",
    "with_server_id.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+----+-----+-----+-----+\n",
      "|server_id|sequence_id|n_data|2500| 5000| 7500|10000|\n",
      "+---------+-----------+------+----+-----+-----+-----+\n",
      "|        a|          0|    32|12.5| 20.7| 32.8| 43.1|\n",
      "|        a|          0|    64|17.0| 38.0| 54.7| 74.4|\n",
      "|        a|          0|   128|37.5| 74.7|141.0|221.2|\n",
      "|        a|          1|    32|12.5| 20.8| 30.5| 39.5|\n",
      "|        a|          1|    64|17.4| 34.9| 54.1| 74.0|\n",
      "|        a|          1|   128|38.9| 76.3|142.0|224.5|\n",
      "|        a|          2|    32|19.0| 24.9| 32.4| 37.0|\n",
      "|        a|          2|    64|23.9| 36.3| 47.4|132.7|\n",
      "|        a|          2|   128|36.0|131.9|215.5|248.8|\n",
      "|        a|          3|    32| 3.6|  3.4|  3.6|  3.6|\n",
      "|        a|          3|    64| 3.7|  3.6|  3.6|  3.7|\n",
      "|        a|          3|   128| 3.6|  3.7|  3.6|  3.9|\n",
      "|        b|          0|    32| 8.6| 12.6| 18.9| 22.2|\n",
      "|        b|          0|    64|13.5| 23.7| 32.6| 41.3|\n",
      "|        b|          0|   128|25.0| 44.1| 67.8| 91.8|\n",
      "|        b|          1|    32| 8.4| 12.6| 17.5| 22.3|\n",
      "|        b|          1|    64|13.2| 22.5| 32.2| 41.8|\n",
      "|        b|          1|   128|25.3| 43.8| 67.0| 91.4|\n",
      "|        b|          2|    32|17.1| 21.4| 26.3| 31.7|\n",
      "|        b|          2|    64|21.8| 31.6| 39.7| 48.6|\n",
      "+---------+-----------+------+----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----------+------+----+-----+-----+\n",
      "|server_id|sequence_id|n_rows|  32|   64|  128|\n",
      "+---------+-----------+------+----+-----+-----+\n",
      "|        a|          0|  2500|12.5| 17.0| 37.5|\n",
      "|        a|          0|  5000|20.7| 38.0| 74.7|\n",
      "|        a|          0|  7500|32.8| 54.7|141.0|\n",
      "|        a|          0| 10000|43.1| 74.4|221.2|\n",
      "|        a|          1|  2500|12.5| 17.4| 38.9|\n",
      "|        a|          1|  5000|20.8| 34.9| 76.3|\n",
      "|        a|          1|  7500|30.5| 54.1|142.0|\n",
      "|        a|          1| 10000|39.5| 74.0|224.5|\n",
      "|        a|          2|  2500|19.0| 23.9| 36.0|\n",
      "|        a|          2|  5000|24.9| 36.3|131.9|\n",
      "|        a|          2|  7500|32.4| 47.4|215.5|\n",
      "|        a|          2| 10000|37.0|132.7|248.8|\n",
      "|        a|          3|  2500| 3.6|  3.7|  3.6|\n",
      "|        a|          3|  5000| 3.4|  3.6|  3.7|\n",
      "|        a|          3|  7500| 3.6|  3.6|  3.6|\n",
      "|        a|          3| 10000| 3.6|  3.7|  3.9|\n",
      "|        b|          0|  2500| 8.6| 13.5| 25.0|\n",
      "|        b|          0|  5000|12.6| 23.7| 44.1|\n",
      "|        b|          0|  7500|18.9| 32.6| 67.8|\n",
      "|        b|          0| 10000|22.2| 41.3| 91.8|\n",
      "+---------+-----------+------+----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_by_n_data = (\n",
    "    with_server_id\n",
    "    .groupBy(\"server_id\", \"sequence_id\", \"n_data\")\n",
    "    .pivot(\"n_rows\")\n",
    "    .min(\"run_seconds\")\n",
    "    .orderBy(\"server_id\", \"sequence_id\", \"n_data\")\n",
    ")\n",
    "\n",
    "task_by_rows = (\n",
    "    with_server_id\n",
    "    .groupBy(\"server_id\", \"sequence_id\", \"n_rows\")\n",
    "    .pivot(\"n_data\")\n",
    "    .min(\"run_seconds\")\n",
    "    .orderBy(\"server_id\", \"sequence_id\", \"n_rows\")\n",
    ")\n",
    "\n",
    "task_by_n_data.show()\n",
    "task_by_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = task_by_n_data.toPandas()\n",
    "df_row = task_by_rows.toPandas()\n",
    "\n",
    "df_n.to_csv(\"2020-08-25-cpu-time-by-n-data.csv\", index=False)\n",
    "df_row.to_csv(\"2020-08-25-cpu-time-by-n-rows.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------\n",
      " cluster_id       | test-32-2500-n1-standard-16-0-20200823234848898312                               \n",
      " application_name | application_1598251821633_0002                                                   \n",
      " submission_time  | 1598251912108                                                                    \n",
      " completion_time  | 1598251915217                                                                    \n",
      " task_id          | 0                                                                                \n",
      " stage_attempt_id | 0                                                                                \n",
      " stage_name       | first at /tmp/df2001cf035740a49970f1deb9035caf/prio_processor.egg/prio_proces... \n",
      " number_of_tasks  | 2                                                                                \n",
      "-RECORD 1--------------------------------------------------------------------------------------------\n",
      " cluster_id       | test-32-2500-n1-standard-16-0-20200823234848898312                               \n",
      " application_name | application_1598251821633_0002                                                   \n",
      " submission_time  | 1598251915241                                                                    \n",
      " completion_time  | 1598251917474                                                                    \n",
      " task_id          | 1                                                                                \n",
      " stage_attempt_id | 0                                                                                \n",
      " stage_name       | first at /tmp/df2001cf035740a49970f1deb9035caf/prio_processor.egg/prio_proces... \n",
      " number_of_tasks  | 1                                                                                \n",
      "-RECORD 2--------------------------------------------------------------------------------------------\n",
      " cluster_id       | test-32-2500-n1-standard-16-0-20200823234848898312                               \n",
      " application_name | application_1598251821633_0002                                                   \n",
      " submission_time  | 1598251918643                                                                    \n",
      " completion_time  | 1598251921080                                                                    \n",
      " task_id          | 2                                                                                \n",
      " stage_attempt_id | 0                                                                                \n",
      " stage_name       | json at NativeMethodAccessorImpl.java:0                                          \n",
      " number_of_tasks  | 2                                                                                \n",
      "-RECORD 3--------------------------------------------------------------------------------------------\n",
      " cluster_id       | test-32-2500-n1-standard-16-0-20200823234848898312                               \n",
      " application_name | application_1598251821633_0002                                                   \n",
      " submission_time  | 1598251921111                                                                    \n",
      " completion_time  | 1598251921387                                                                    \n",
      " task_id          | 3                                                                                \n",
      " stage_attempt_id | 0                                                                                \n",
      " stage_name       | json at NativeMethodAccessorImpl.java:0                                          \n",
      " number_of_tasks  | 200                                                                              \n",
      "-RECORD 4--------------------------------------------------------------------------------------------\n",
      " cluster_id       | test-32-2500-n1-standard-16-0-20200823234848898312                               \n",
      " application_name | application_1598251821633_0002                                                   \n",
      " submission_time  | 1598251921600                                                                    \n",
      " completion_time  | 1598251922924                                                                    \n",
      " task_id          | 2                                                                                \n",
      " stage_attempt_id | 1                                                                                \n",
      " stage_name       | json at NativeMethodAccessorImpl.java:0                                          \n",
      " number_of_tasks  | 1                                                                                \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage_info = (\n",
    "    job_history.where(F.col(\"Event\") == \"SparkListenerStageCompleted\")\n",
    "    .withColumn(\"job_info\", parse_benchmark_path(\"filename\"))\n",
    "    .select(\n",
    "        \"job_info.*\",\n",
    "        F.col(\"Stage Info\").getField(\"Submission Time\").alias(\"submission_time\"),\n",
    "        F.col(\"Stage Info\").getField(\"Completion Time\").alias(\"completion_time\"),\n",
    "        F.col(\"Stage Info\").getField(\"Stage ID\").alias(\"task_id\"),\n",
    "        F.col(\"Stage Info\").getField(\"Stage Attempt ID\").alias(\"stage_attempt_id\"),\n",
    "        F.col(\"Stage Info\").getField(\"Stage Name\").alias(\"stage_name\"),\n",
    "        F.col(\"Stage Info\").getField(\"Number of Tasks\").alias(\"number_of_tasks\"),\n",
    "    )\n",
    ")\n",
    "stage_info.show(vertical=True, n=5, truncate=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37564bitvenvvenv84bb2f0525ce41e0ba9acedc87af5f85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
