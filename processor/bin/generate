#!/bin/bash
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.

# This scripts generates data for testing the pipeline. The data is generated
# based on the data configuration file.

set -euo pipefail
set -x

: "${DATA_CONFIG}"
: "${PUBLIC_KEY_HEX_INTERNAL?}"
: "${PUBLIC_KEY_HEX_EXTERNAL?}"
: "${BUCKET_INTERNAL_PRIVATE?}"
: "${BUCKET_EXTERNAL_PRIVATE?}"
: "${GOOGLE_APPLICATION_CREDENTIALS?:=}"

function authenticate() {
    local cred=${GOOGLE_APPLICATION_CREDENTIALS}
    local test_bucket=${BUCKET_INTERNAL_PRIVATE}

    if [[ -n "${cred}" ]]; then
        gcloud auth activate-service-account --key-file "${cred}"
    else
        # https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform
        echo "No JSON credentials provided, using default scopes."
    fi
    gsutil ls "gs://${test_bucket}"
}

function rsync() {
    local server_id=$1
    local bucket=$2
    local dest=gs://${bucket}/raw/

    gsutil -m rsync -r "server_${server_id}/raw/" "${dest}"
    touch _SUCCESS
    gsutil cp _SUCCESS "${dest}"
}

function config_keys() {
    : "${DATA_CONFIG?}"
    jq -r 'keys | join("\n")' "${DATA_CONFIG}"
}

function config_get() {
    : "${DATA_CONFIG?}"
    local key=$1
    jq -r ".\"${key}\"" "${DATA_CONFIG}"
}

function generate_data() {
    local n_data=$1
    python -c "print([int(x % 3 == 0 or x % 5 == 0) for x in range(${n_data})])"
}


function generate_dataset() {
    local batch_id=$1
    local n_data
    n_data=$(config_get "$batch_id")
    if [[ $n_data == "null" ]]; then
        echo "unknown dimensions for ${batch_id}"
        n_data=10
    fi

    local date
    date=$(date +%Y-%m-%d)

    local out_a="server_a/raw/submission_date=${date}/batch_id=${batch_id}"
    local out_b="server_b/raw/submission_date=${date}/batch_id=${batch_id}"
    mkdir -p "${out_a}"
    mkdir -p "${out_b}"

    for i in {1..5}; do
        filename="${batch_id}-part-$i.json"
        set +x
        for ((j=0; j < i; j++)); do
            generate_data "${n_data} ">> "${filename}"
        done
        set -x
        prio encode-shares \
            --input "${filename}" \
            --batch-id "${batch_id}" \
            --n-data "${n_data}" \
            --output-A "${out_a}" \
            --output-B "${out_b}"

        # test for robustness by inserting an invalid entry into the shares
        set +x
        echo '{"id": "asdf", "payload": "asdf"}' >> "${out_a}/${filename}"
        echo '{"id": "asdf", "payload": "asdf"}' >> "${out_b}/${filename}"
        set -x
    done
}

function main() {
    cd /tmp
    authenticate

    export DATA_CONFIG          # used by config_get
    export -f config_get        # used by generate_dataset
    export -f generate_data     # used by generate_dataset
    export -f generate_dataset
    parallel generate_dataset ::: "$(config_keys)"

    # also generate a batch-id with unknown dimensions
    generate_dataset "bad-id"

    rsync a "${BUCKET_INTERNAL_PRIVATE}"
    rsync b "${BUCKET_EXTERNAL_PRIVATE}"
}

if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
