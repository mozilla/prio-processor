#!/bin/bash
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.

# This scripts defines the batched processing pipeline for use on Google Cloud
# Platform (GCP).

set -euo pipefail
set -x

# Re-export variables for GNU parallel 20160222. Newer versions of parallel
# contain a env_parallel script for exporting the current environment.

# Avoid printing sensitive environment data into the logs.
set +x
export SHARED_SECRET=${SHARED_SECRET?}
export PRIVATE_KEY_HEX=${PRIVATE_KEY_HEX?}
set -x
export SERVER_ID=${SERVER_ID?}
export PUBLIC_KEY_HEX_INTERNAL=${PUBLIC_KEY_HEX_INTERNAL?}
export PUBLIC_KEY_HEX_EXTERNAL=${PUBLIC_KEY_HEX_EXTERNAL?}
export DATA_CONFIG=${DATA_CONFIG?}

# Variables in the following block do not need to be re-exported.

# Error if variables are unset.
: "${BUCKET_INTERNAL_PRIVATE?}"
: "${BUCKET_INTERNAL_SHARED?}"
: "${BUCKET_EXTERNAL_SHARED?}"

# Assign to a default if variables are unset.
: "${GOOGLE_APPLICATION_CREDENTIALS:=}"
: "${RETRY_LIMIT:=5}"
: "${RETRY_DELAY:=2}"
: "${RETRY_BACKOFF_EXPONENT:=2}"


function create_folders() {
    mkdir -p raw
    mkdir -p intermediate/internal/verify1
    mkdir -p intermediate/external/verify1
    mkdir -p intermediate/internal/verify2
    mkdir -p intermediate/external/verify2
    mkdir -p intermediate/internal/aggregate
    mkdir -p intermediate/external/aggregate
    mkdir -p processed
}


function send_output_external() {
    # Copy data generated by a processing step into the receiving bucket of the
    # co-processing server. A _SUCCESS file is generated on a successful copy.

    : "${BUCKET_EXTERNAL_SHARED?}"          # bucket of the external server
    local output_internal=$1                # path to internal output
    local output_external=$2                # relative path to external output

    local path="${BUCKET_EXTERNAL_SHARED}/${output_external}"
    gsutil -m rsync -r "${output_internal}/" "${path}/"
    touch _SUCCESS
    gsutil cp _SUCCESS "${path}/"
}


function send_output_internal() {
    # Copy local data to a remote bucket that is accessible to the current
    # server.

    : "${BUCKET_INTERNAL_PRIVATE?}"         # bucket of internal server
    local output=$1                         # relative path to output

    local path="${BUCKET_INTERNAL_PRIVATE}/${output}"
    gsutil -m rsync -r "${output}/" "${path}/"
    touch _SUCCESS
    gsutil cp _SUCCESS "${path}/"
}


function wait_for_data() {
    # Wait for a completed batch of data, signaled by the appearance of a
    # _SUCCESS file.

    : "${RETRY_LIMIT?}"                 # number of times to retry
    : "${RETRY_DELAY?}"                 # the initial delay between retries
    : "${RETRY_BACKOFF_EXPONENT?}"      # the exponent for exponential backoff
    local path=$1                       # absolute path of file to stat

    local retries=0
    local backoff=${RETRY_DELAY}
    set +e
    while ! gsutil -q stat "${path}"; do
        sleep ${backoff};
        ((backoff *= RETRY_BACKOFF_EXPONENT))
        ((retries++))
        if [[ "${retries}" -gt "${RETRY_LIMIT}" ]]; then
            echo "Reached the maximum number of retries."
            exit 1
        fi
    done
    set -e
}


function fetch_input_blocked() {
    # Poll for a success file and then copy the appropriate files locally.

    local bucket=$1     # input bucket
    local input=$2      # relative path to input data

    local path="${bucket}/${input}"
    wait_for_data "${path}/_SUCCESS"
    gsutil -m rsync -r "${path}/" "${input}/"
}


function fetch_input_private() {
    # Wait for data to be written to the internal private bucket before copying.

    : "${BUCKET_INTERNAL_PRIVATE?}"     # bucket for storing intermediate data
    local input=$1                      # relative path of private input data

    fetch_input_blocked "${BUCKET_INTERNAL_PRIVATE}" "${input}"
}


function fetch_input_shared() {
    # Wait for data to be written to the internal shared bucket before copying.

    : "${BUCKET_INTERNAL_SHARED?}"      # bucket for receiving messages
    local input=$1                      # relative path of shared input data

    fetch_input_blocked "${BUCKET_INTERNAL_SHARED}" "${input}"
}


function list_partitions() {
    # List partitions that are ready for processing.

    local input=$1

    local skip_front
    # number of leading folders to skip is the directory depth + 1
    skip_front=$(echo "${input} " | awk '{ split($0, x, "/"); print(length(x) + 1) }')
    # print relative paths to input that contain json data
    find "${input}" -type f -name "*.json" | cut -sd / -f "${skip_front}-"
}

function config_get() {
    # Get a value from the data configuration file.

    : "${DATA_CONFIG?}"
    local batch_id=$1

    jq -r ".\"${batch_id}\"" "${DATA_CONFIG}"
}


function extract_batch_id() {
    # Extract the batch-id partition key from a hadoop-style path.

    local path=$1   # hadoop-style path with `/{key}={value}/` partitions

    local code='import re; m=re.search(r"batch_id=([^/]*)", input()); print(m[1])'
    echo "$path" | python3 -c "$code"
}


function export_data_dimensions() {
    # Export data dimensions for processing prio data.
    #
    #  This involves extracting the batch-id from the partition key encoded in
    # the path, and then using the key to find the length of te bit-vector in
    # the static configuration file.

    : "${DATA_CONFIG?}"     # used by `extract_batch_id`, contains dimensions
    local path=$1           # hadoop-style path

    BATCH_ID=$(extract_batch_id "${path}")
    N_DATA=$(config_get "${BATCH_ID}")

    export BATCH_ID
    export N_DATA
}


function verify1() {
    # Read the shares for this server and start verification with the external server.

    export input="raw"
    export output_internal="intermediate/internal/verify1"
    export output_external="intermediate/external/verify1"

    fetch_input_private ${input}

    function process() {
        local filename=$1
        export_data_dimensions "$filename"
        if [[ $N_DATA == "null" ]]; then
            echo "$BATCH_ID has unknown dimension, skipping..."
            return
        fi

        prio verify1 \
            --input "${input}/${filename}" \
            --output "${output_internal}/$(dirname "${filename}")"
    }
    export -f process
    parallel process ::: "$(list_partitions ${input})"

    send_output_external ${output_internal} ${output_external}
}


function verify2() {
    # Verify that the shares are well-formed by proving a secret-shared non-interactive proof.

    export input="raw"
    export input_internal="intermediate/internal/verify1"
    export input_external="intermediate/external/verify1"
    export output_internal="intermediate/internal/verify2"
    export output_external="intermediate/external/verify2"

    fetch_input_shared ${input_external}

    function process() {
        local filename=$1
        export_data_dimensions "$filename"
        if [[ $N_DATA == "null" ]]; then
            echo "$BATCH_ID has unknown dimension, skipping..."
            return
        fi

        prio verify2 \
            --input "${input}/${filename}" \
            --input-internal "${input_internal}/${filename}" \
            --input-external "${input_external}/${filename}" \
            --output "${output_internal}/$(dirname "${filename}")"
    }
    export -f process
    parallel process ::: "$(list_partitions ${input_internal})"

    send_output_external ${output_internal} ${output_external}
}


function aggregate() {
    # Accumulate well-formed shares.

    export input="raw"
    export input_internal="intermediate/internal/verify2"
    export input_external="intermediate/external/verify2"
    export output_internal="intermediate/internal/aggregate"
    export output_external="intermediate/external/aggregate"

    fetch_input_shared ${input_external}

    function process() {
        local filename=$1
        export_data_dimensions "$filename"
        if [[ $N_DATA == "null" ]]; then
            echo "$BATCH_ID has unknown dimension, skipping..."
            return
        fi

        prio aggregate \
            --input "${input}/${filename}" \
            --input-internal "${input_internal}/${filename}" \
            --input-external "${input_external}/${filename}" \
            --output "${output_internal}/$(dirname "${filename}")"
    }
    export -f process
    parallel process ::: "$(list_partitions ${input_internal})"

    send_output_external ${output_internal} ${output_external}
}


function publish() {
    # Publish the aggregated shares.

    export input_internal="intermediate/internal/aggregate"
    export input_external="intermediate/external/aggregate"
    export output="processed"

    fetch_input_shared ${input_external}

    function process() {
        local filename=$1
        export_data_dimensions "$filename"
        if [[ $N_DATA == "null" ]]; then
            echo "$BATCH_ID has unknown dimension, skipping..."
            return
        fi

        prio publish \
            --input-internal "${input_internal}/${filename}" \
            --input-external "${input_external}/${filename}" \
            --output "${output}/$(dirname "${filename}")"
    }
    export -f process
    parallel process ::: "$(list_partitions ${input_internal})"

    send_output_internal ${output}
}

function main() {
    "${BASH_SOURCE%/*}/authenticate"

    gsutil ls "${BUCKET_INTERNAL_PRIVATE?}"
    gsutil ls "${BUCKET_INTERNAL_SHARED?}"
    gsutil ls "${BUCKET_EXTERNAL_SHARED?}"

    cd /tmp && create_folders

    # export functions necessary for parallel
    export -f extract_batch_id
    export -f config_get
    export -f export_data_dimensions

    verify1
    verify2
    aggregate
    publish
}

if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
