#!/bin/bash
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.

# This scripts defines the batched processing pipeline for use on Google Cloud
# Platform (GCP).

set -euo pipefail
set -x

# Re-export variables for the prio-spark commands.
# Avoid printing sensitive environment data into the logs.
set +x
export SHARED_SECRET=${SHARED_SECRET?}
export PRIVATE_KEY_HEX=${PRIVATE_KEY_HEX?}
set -x
export SERVER_ID=${SERVER_ID?}
export PUBLIC_KEY_HEX_INTERNAL=${PUBLIC_KEY_HEX_INTERNAL?}
export PUBLIC_KEY_HEX_EXTERNAL=${PUBLIC_KEY_HEX_EXTERNAL?}
export DATA_CONFIG=${DATA_CONFIG?}

# Variables in the following block do not need to be re-exported.

# Error if variables are unset.
: "${BUCKET_INTERNAL_PRIVATE?}"
: "${BUCKET_INTERNAL_SHARED?}"
: "${BUCKET_EXTERNAL_SHARED?}"

# Assign to a default if variables are unset.
: "${GOOGLE_APPLICATION_CREDENTIALS:=}"
: "${RETRY_LIMIT:=5}"
: "${RETRY_DELAY:=2}"
: "${RETRY_BACKOFF_EXPONENT:=2}"

# Global variables pointing for managing Spark
BOOTSTRAP_BUCKET=${BOOTSTRAP_BUCKET:-$(mktemp -d -t bootstrap-XXX)}

function spark_submit() {
    spark-submit \
        --py-files "${BOOTSTRAP_BUCKET}/prio_processor.egg" \
        "${BOOTSTRAP_BUCKET}/processor-spark.py" \
        "$@"
}

function rsync_delete() {
    # Copy local data to a remote bucket that is accessible to the current
    # server, removing files on the recieving end.

    local src=$1
    local dst=$2

    # If we are working with a local directory, we generate the parent tree
    # by copying a tempory file into the prefix.
    touch _TEMP
    gsutil cp _TEMP "${dst}/_TEMP"

    gsutil -m rsync -d -r "${src}/" "${dst}/"
    touch _SUCCESS
    gsutil cp _SUCCESS "${dst}/"
}

function wait_for_data() {
    # Wait for a completed batch of data, signaled by the appearance of a
    # _SUCCESS file.

    : "${RETRY_LIMIT?}"            # number of times to retry
    : "${RETRY_DELAY?}"            # the initial delay between retries
    : "${RETRY_BACKOFF_EXPONENT?}" # the exponent for exponential backoff
    local path=$1                  # absolute path of file to stat

    local retries=0
    local backoff=${RETRY_DELAY}
    set +e
    while ! gsutil -q stat "${path}"; do
        sleep ${backoff}
        ((backoff *= RETRY_BACKOFF_EXPONENT))
        ((retries++))
        if [[ "${retries}" -gt "${RETRY_LIMIT}" ]]; then
            echo "Reached the maximum number of retries."
            exit 1
        fi
    done
    set -e
}

function config_get() {
    # Get a value from the data configuration file.

    : "${DATA_CONFIG?}"
    local batch_id=$1
    local field=$2

    jq -r ".[] | select(.batch_id==\"${batch_id}\") | .${field}" "${DATA_CONFIG}"
}

function verify1() {
    # Read the shares for this server and start verification with the external server.

    local input="${BUCKET_INTERNAL_PRIVATE}/raw"
    local output_internal="${BUCKET_INTERNAL_SHARED}/intermediate/internal/verify1"
    local output_external="${BUCKET_EXTERNAL_SHARED}/intermediate/external/verify1"

    wait_for_data "${input}/_SUCCESS"

    # TODO: move batch_id into Spark logic. The reason that this isn't
    # implemnted currently is because the potential for a batch_ids to belong to
    # different public keys. One downside is that data-skew might significantly
    # affect the job.
    for batch_id in $(jq -r '.[].batch_id' "${DATA_CONFIG}"); do
        prefix="batch_id=${batch_id}"
        n_data=$(config_get "${batch_id}" "n_data")
        spark_submit verify1 \
            --batch-id "${batch_id}" \
            --n-data "${n_data}" \
            --input "${input}/${prefix}" \
            --output "${output_internal}/${prefix}"
    done

    rsync_delete "${output_internal}" "${output_external}"
}

function verify2() {
    # Verify that the shares are well-formed by proving a secret-shared non-interactive proof.

    local input="${BUCKET_INTERNAL_PRIVATE}/raw"
    local input_internal="${BUCKET_INTERNAL_SHARED}/intermediate/internal/verify1"
    local input_external="${BUCKET_INTERNAL_SHARED}/intermediate/external/verify1"
    local output_internal="${BUCKET_INTERNAL_SHARED}/intermediate/internal/verify2"
    local output_external="${BUCKET_EXTERNAL_SHARED}/intermediate/external/verify2"

    wait_for_data "${input_external}/_SUCCESS"

    for batch_id in $(jq -r '.[].batch_id' "${DATA_CONFIG}"); do
        prefix="batch_id=${batch_id}"
        n_data=$(config_get "${batch_id}" "n_data")
        spark_submit verify2 \
            --batch-id "${batch_id}" \
            --n-data "${n_data}" \
            --input "${input}/${prefix}" \
            --input-internal "${input_internal}/${prefix}" \
            --input-external "${input_external}/${prefix}" \
            --output "${output_internal}/${prefix}"
    done

    rsync_delete "${output_internal}" "${output_external}"
}

function aggregate() {
    # Accumulate well-formed shares.

    local input="${BUCKET_INTERNAL_PRIVATE}/raw"
    local input_internal="${BUCKET_INTERNAL_SHARED}/intermediate/internal/verify2"
    local input_external="${BUCKET_INTERNAL_SHARED}/intermediate/external/verify2"
    local output_internal="${BUCKET_INTERNAL_SHARED}/intermediate/internal/aggregate"
    local output_external="${BUCKET_EXTERNAL_SHARED}/intermediate/external/aggregate"

    wait_for_data "${input_external}/_SUCCESS"

    for batch_id in $(jq -r '.[].batch_id' "${DATA_CONFIG}"); do
        prefix="batch_id=${batch_id}"
        n_data=$(config_get "${batch_id}" "n_data")
        spark_submit aggregate \
            --batch-id "${batch_id}" \
            --n-data "${n_data}" \
            --input "${input}/${prefix}" \
            --input-internal "${input_internal}/${prefix}" \
            --input-external "${input_external}/${prefix}" \
            --output "${output_internal}/${prefix}"
    done

    rsync_delete "${output_internal}" "${output_external}"
}

function publish() {
    # Publish the aggregated shares.

    local input_internal="${BUCKET_INTERNAL_SHARED}/intermediate/internal/aggregate"
    local input_external="${BUCKET_INTERNAL_SHARED}/intermediate/external/aggregate"
    local output="${BUCKET_INTERNAL_SHARED}/processed"

    wait_for_data "${input_external}/_SUCCESS"

    for batch_id in $(jq -r '.[].batch_id' "${DATA_CONFIG}"); do
        prefix="batch_id=${batch_id}"
        n_data=$(config_get "${batch_id}" "n_data")
        spark_submit publish \
            --batch-id "${batch_id}" \
            --n-data "${n_data}" \
            --input-internal "${input_internal}/${prefix}" \
            --input-external "${input_external}/${prefix}" \
            --output "${output}/${prefix}"
    done
}

function main() {
    "${BASH_SOURCE%/*}/authenticate"
    source "${BASH_SOURCE%/*}/dataproc"
    SUBMODULE="spark" bootstrap "${BOOTSTRAP_BUCKET}"

    working=$(mktemp -d -t process-XXX)
    cd "${working}"

    verify1
    verify2
    aggregate
    publish
}

if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
